{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changlinli/cuda-colab-experiments/blob/main/Copy_of_gpu_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "\n",
        "Here are a few resources that might be helpful if you are stuck/confused at any point.\n",
        "\n",
        "https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf is the comprehensive guide to writing CUDA code and kernels. You can refer to this to figure out the documentation/signatures of functions.\n",
        "\n",
        "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html is the documentation for NCCL (NVIDIA Collective Communication Library). Refer to this for documentation about inter-GPU communication. We are using a patched version of NCCL, which allows us to pretend we are using multiple GPUs while we have only one for testing and learning without having to actually get a multi-GPU setup.\n",
        "\n",
        "### Before you start\n",
        "\n",
        "If you are using google colab, ensure that you are connected to a GPU runtime by selecting Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "\n",
        "Switch to [this notebook](https://colab.research.google.com/drive/1e1xO6c1GNGOcNRLuYd4SIuY45Fq7FYZl) for the python version of this notebook - it makes debugging a lot harder though, and overall gives you less control over the kernels you write."
      ],
      "metadata": {
        "id": "ECdcXxkLW81G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caveats\n",
        "- we will be using python files instead of jupyter notebooks because CUDA programs can cause process to hang and be unresponsive. If this happens, you can find the offending process' ID using `nvidia-smi` or `ps aux`, and terminate it with `kill -9 <pid>`."
      ],
      "metadata": {
        "id": "eEDsowtsY2nG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding two vectors (and toolchain setup)\n",
        "\n",
        "We will start off with a simple kernel to add two 1D torch tensors. First, we need to write the kernel with C++, which will be compiled by nvcc, nvidia's C compiler. Let's call this `add_kernel.cu` (the next cell explains what's going on in more detail)\n",
        "\n",
        "```cpp\n",
        "#include <stdio.h>\n",
        "extern \"C\" { // to prevent C++ name mangling, we use extern \"C\"\n",
        "    __global__ void add_cuda_kernel(float *a, float *b, float *c, int size) {\n",
        "        int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "        if (i < size) {\n",
        "            c[i] = a[i] + b[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    void add_cuda(float *a, float *b, float *c, int size, cudaStream_t *stream) {\n",
        "        dim3 blockDim(256);\n",
        "        dim3 gridDim((size + blockDim.x - 1) / blockDim.x);\n",
        "        add_cuda_kernel<<<gridDim, blockDim, 0, *stream>>>(a, b, c, size);\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "We also need driver code to create tensors and call our kernel:\n",
        "```python\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import ctypes\n",
        "import subprocess\n",
        "\n",
        "kernel_file = \"add_kernel.cu\"\n",
        "\n",
        "# Compile the CUDA kernel using nvcc\n",
        "nvcc_command = \"nvcc -shared --compiler-options '-fPIC' -o kernel.so \" + kernel_file + \" -I /usr/include/torch/csrc/api/include/\"\n",
        "result = subprocess.run(nvcc_command, shell=True, text=True, capture_output=True)\n",
        "\n",
        "if result.returncode != 0:\n",
        "    print(f\"Error: nvcc compilation failed\")\n",
        "    print(result.stderr.strip())\n",
        "    exit(1)\n",
        "else:\n",
        "    print(f\"compilation successful\")\n",
        "\n",
        "# Initialize CUDA variables\n",
        "device = torch.device(\"cuda\")\n",
        "size = 1024\n",
        "\n",
        "# Allocate and assign input tensors in GPU memory\n",
        "a = torch.ones(size, device=device)\n",
        "b = torch.ones(size, device=device)\n",
        "c = torch.zeros(size, device=device)\n",
        "\n",
        "# Load the custom kernel\n",
        "lib = ctypes.CDLL(f\"{os.getcwd()}/kernel.so\")\n",
        "\n",
        "# Define the argument types and return type for the function\n",
        "lib.add_cuda.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.POINTER(ctypes.c_void_p)]\n",
        "lib.add_cuda.restype = None\n",
        "\n",
        "stream = torch.cuda.current_stream()\n",
        "\n",
        "print(\"calling our kernel...\")\n",
        "# Call the custom CUDA kernel\n",
        "lib.add_cuda(a.data_ptr(), b.data_ptr(), c.data_ptr(), int(size), ctypes.c_void_p(stream.cuda_stream))\n",
        "\n",
        "# Synchronize the CUDA stream\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Print the result\n",
        "print(c)\n",
        "\n",
        "```\n",
        "\n",
        "To run our kernel, we can now just run the python file. It will compile the kernel (with nvcc), load it, and call it. The output you see should be a bunch of twos, because 1 + 1 = 2. Try tweaking the code to have different input sizes.\n",
        "\n",
        "Exercise: how would you add two, or multi dimensional tensors?"
      ],
      "metadata": {
        "id": "Syb80CRIZ6sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python driver.py"
      ],
      "metadata": {
        "id": "eI1Fu0fE-42B",
        "outputId": "3912833f-6f29-4fc2-9e4c-e435355f9563",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compilation successful\n",
            "calling our kernel...\n",
            "tensor([2., 2., 2.,  ..., 2., 2., 2.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding two vectors: explained\n",
        "Here's what each line in our kernel does - most of this was written by GPT, which seems like a pretty good way to figure out what something is doing if you are confused. Look at [nvidia's documentation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#c-language-extensions) for more detailed description of their C/C++ API.\n",
        "\n",
        "### add_kernel.cu\n",
        "\n",
        "`#include <stdio.h>`\n",
        "\n",
        "The preprocessor directive #include tells the compiler to include the contents of the standard input/output header file (stdio.h) in the program. stdio.h allows us to use printf for debugging.\n",
        "\n",
        "`extern \"C\" {`\n",
        "\n",
        "This line specifies that the following functions should be treated as C functions, which are not name-mangled by the C++ compiler. This is important for interoperability between CUDA and other languages, such as calling CUDA kernels from Python using ctypes or similar mechanisms.\n",
        "\n",
        "`__global__ void add_cuda_kernel(float *a, float *b, float *c, int size) {`\n",
        "\n",
        "This line defines a CUDA kernel function named \"add_cuda_kernel\" with a \\_\\_global\\_\\_ attribute, meaning this function can be called from the host (CPU) and executed on the device (GPU). The function takes four parameters: three float pointers to our tensors a, b, and c, and an integer size.\n",
        "\n",
        "`int i = threadIdx.x + blockIdx.x * blockDim.x;`\n",
        "\n",
        "This line computes the current thread's index `i` based on the thread's x-coordinate `threadIdx.x` within a block, the block's x-coordinate `blockIdx.x` within the grid, and the block's size `blockDim.x` along the x-axis. This approach is used to handle one-dimensional array indexing in parallel.\n",
        "\n",
        "`if (i < size) {`\n",
        "\n",
        "This if statement checks if the current index `i` is within the bounds of the input array size.\n",
        "\n",
        "`c[i] = a[i] + b[i];`\n",
        "\n",
        "If the index is within bounds, this line performs an element-wise addition of the corresponding elements in arrays a and b and stores the result in array c.\n",
        "\n",
        "`void add_cuda(float *a, float *b, float *c, int size, cudaStream_t *stream) {`\n",
        "\n",
        "This line declares a C function called add_cuda that takes five parameters: three float pointers (a, b, and c), an integer (size), and a cudaStream_t pointer (stream).\n",
        "\n",
        "`dim3 blockDim(256);`\n",
        "\n",
        "This line creates a dim3 object called blockDim and initializes it with the dimensions (256, 1, 1) for the block. The block has 256 threads along the x-axis and a single thread along the y and z axes.\n",
        "\n",
        "`dim3 gridDim((size + blockDim.x - 1) / blockDim.x);`\n",
        "\n",
        "This line computes the grid dimensions, based on the input array size and block dimensions. It calculates the number of blocks, rounding up to cover the entire array by adding (blockDim.x - 1) before dividing.\n",
        "\n",
        "`add_cuda_kernel<<<gridDim, blockDim, 0, *stream>>>(a, b, c, size);`\n",
        "\n",
        "This line launches the add_cuda_kernel on the GPU with the specified grid and block dimensions, no shared memory (0), and the given stream. The kernel is passed the input parameters a, b, c, and size."
      ],
      "metadata": {
        "id": "x-kVfW9OXnXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debugging (optional, set this up when you need it)\n",
        "Before actually using a debugger, try logging stuff with `printf(\"%d\\n\", some_value)`. If you have segmentation faults, compile the kernel with libSegfault:\n",
        "```python\n",
        "nvcc_command = \"nvcc -shared --compiler-options '-fPIC -lSegFault' -o kernel.so \" + kernel_file + \" -I /usr/include/torch/csrc/api/include/\"\n",
        "```\n",
        "This should tell you where exactly your program is segfaulting.\n",
        "\n",
        "### cuda-gdb\n",
        "\n",
        "You can use `cuda-gdb` to attach a debugger to your program:\n",
        "\n",
        "0. Modify the nvcc command in your python code to include debug symbols, disable optimization, and include lineinfo:\n",
        "```python\n",
        "nvcc_command = \"nvcc -g -G -O0 -lineinfo -shared --compiler-options '-fPIC -lSegFault' -o kernel.so \" + kernel_file + \" -I /usr/include/torch/csrc/api/include/\"\n",
        "```\n",
        "\n",
        "1. Launch `cuda-gdb`:\n",
        "```\n",
        "PYTHON_DISABLE_OPTIMIZE=1 cuda-gdb python\n",
        "(gdb) set args path/to/your/python_script.py\n",
        "```\n",
        "\n",
        "2. Set a breakpoint in your CUDA kernel code by specifying the source file name and the line number:\n",
        "```\n",
        "(gdb) break your_cuda_source.cu:line_number\n",
        "```\n",
        "Replace `your_cuda_source.cu` and `line_number` with the actual file name and line number where you want to set the breakpoint (e.g., `break my_kernel.cu:99`). Make sure you set the breakpoint before running the Python script.\n",
        "\n",
        "3. Run your Python script:\n",
        "```\n",
        "(gdb) run\n",
        "```\n",
        "\n",
        "4. `cuda-gdb` should now break at the specified line number in your CUDA kernel code. You can debug your kernel code using the various `gdb` commands:\n",
        "- Step through the code with `next` (`n`) or `step` (`s`).\n",
        "- Set breakpoints using `break` (`b`). For example, `break my_cuda_file.cu:123` sets a breakpoint at line 123 in `my_cuda_file.cu`.\n",
        "- Examine variable values with `print` (`p`), such as `print array[0]`.\n",
        "- Use `info cuda kernels` to display information about active kernels and their launches.\n",
        "- Use `info cuda devices` to display information about CUDA devices.\n",
        "- Use `info cuda loc` to show the current kernel location.\n",
        "\n",
        "Remember that due to the asynchronous nature of CUDA kernels, you must [use breakpoints](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#breakpoint-function) explicitly in kernel code to pause execution at specific points.\n"
      ],
      "metadata": {
        "id": "jiiHoN_dKJ1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Syntactic Sugar\n",
        "\n",
        "Loading the kernel, specifying the return types, getting the pointers to your data and calling the kernel, and synchronizing the stream can get annoying. So, if you'd like some syntactic sugar, you can run the cell below to get a decorator, `@kernel_function` that allows you to get away with simply specifying the function name and signature of your C++ function. So,\n",
        "```python\n",
        "# Load the custom kernel\n",
        "lib = ctypes.CDLL(f\"{os.getcwd()}/kernel.so\")\n",
        "\n",
        "# Define the argument types and return type for the function\n",
        "lib.add_cuda.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int, ctypes.POINTER(ctypes.c_void_p)]\n",
        "lib.add_cuda.restype = None\n",
        "\n",
        "stream = torch.cuda.current_stream()\n",
        "\n",
        "print(\"calling our kernel...\")\n",
        "# Call the custom CUDA kernel\n",
        "lib.add_cuda(a.data_ptr(), b.data_ptr(), c.data_ptr(), int(size), ctypes.c_void_p(stream.cuda_stream))\n",
        "\n",
        "# Synchronize the CUDA stream\n",
        "torch.cuda.synchronize()\n",
        "```\n",
        "can be written as\n",
        "```python\n",
        "@kernel_function(lib_path=f\"{os.getcwd()}/kernel.so\")\n",
        "def sum_cuda(lib, a: Tensor, c: Tensor, size: int, stream: torch.cuda.streams.Stream) -> None:\n",
        "    return lib.add_cuda\n",
        "\n",
        "sum_cuda(a, c, size)\n",
        "```\n",
        "\n",
        "After this, every call to sum_cuda automatically does all the heavy-lifting for you."
      ],
      "metadata": {
        "id": "lJ6E-9FnsNap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "exec(requests.get('https://gist.githubusercontent.com/pranavgade20/c629d4134f5cc6998b489892b3f90a1b/raw/fb8dc248219c8a60e1cbe6079bdec8d7cfbc777a/kernel_function_decorator.py').text)"
      ],
      "metadata": {
        "id": "TvWj90EFsudL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Atomic Operations\n",
        "\n",
        "Consider how you'd implement a kernel that sums all elements in a tensor - multiple threads writing to the same memory location can lead to race conditions. This is when multiple threads attempt to perform operations on some memory location, and different orders of operation can result in different results.\n",
        "\n",
        "For example, consider two threads, A and B, attempting to increment a variable:\n",
        "[![](https://mermaid.ink/img/pako:eNqFkTFrwzAQhf-KODAeGkO9Chqw6dIhSzNk0SKkaytandzLeQjG_72XiJbSEKpJvO_pnXi3QCgRwULTLImSWLO08oYZW9tGz-_tujaNI0dH_JyRAj4m_8o-OzJ6Js-SQpo8iRmupR3mwqdrfaxSxd12ezdY84w-mlBmEmTzYPr7v57xpmdQrAlPFFg_rvlSTN9XNiobb7ChU9jVCdYcOAn-Tv9J-M8FG8jI2aeoPS7nVw4uHTqwej3X6MDRqj4_S9mfKIAVnnED8xS9fHcK9sV_HFXFmKTwri7msp_1C1Uuhvw?type=png)](https://mermaid.live/edit#pako:eNqFkTFrwzAQhf-KODAeGkO9Chqw6dIhSzNk0SKkaytandzLeQjG_72XiJbSEKpJvO_pnXi3QCgRwULTLImSWLO08oYZW9tGz-_tujaNI0dH_JyRAj4m_8o-OzJ6Js-SQpo8iRmupR3mwqdrfaxSxd12ezdY84w-mlBmEmTzYPr7v57xpmdQrAlPFFg_rvlSTN9XNiobb7ChU9jVCdYcOAn-Tv9J-M8FG8jI2aeoPS7nVw4uHTqwej3X6MDRqj4_S9mfKIAVnnED8xS9fHcK9sV_HFXFmKTwri7msp_1C1Uuhvw)\n",
        "\n",
        "Here, because B read before A had written the result back, B thinks counter = 10, when it should actually be 11. What we need is a way to make sure the operations are *atomic*. That is, no other threads can interrupt the operation in the middle of its execution.\n",
        "\n",
        "CUDA has a few [atomic operations](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions), designed to prevent race conditions and maintain data integrity when multiple threads attempt to access or modify the same memory location simultaneously.\n",
        "\n",
        "Implement a kernel that sums a tensor using the `atomicAdd` function."
      ],
      "metadata": {
        "id": "6nemGnmeZ4eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to setup the code\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import ctypes\n",
        "import subprocess\n",
        "import functools\n",
        "\n",
        "kernel_file = \"add_kernel.cu\"\n",
        "\n",
        "# Compile the CUDA kernel using nvcc\n",
        "def create_nvcc_command(kernel_file_without_cu: str) -> str:\n",
        "  kernel_file_cu = kernel_file_without_cu + \".cu\"\n",
        "  kernel_file_o = kernel_file_without_cu + \".o\"\n",
        "  return f\"nvcc -shared --compiler-options '-fPIC' -o {kernel_file_o} {kernel_file_cu} -I /usr/include/torch/csrc/api/include/\"\n",
        "\n",
        "def compile_with_nvcc(kernel_file_without_cu: str):\n",
        "  return subprocess.run(create_nvcc_command(kernel_file_without_cu), shell=True, text=True, capture_output=True)\n",
        "\n",
        "compile_with_nvcc(\"add_kernel.cu\")\n",
        "\n",
        "# A nice little decorator to compile the program as well as run it\n",
        "def make_kernel_function(kernel_file_without_cu: str):\n",
        "  def decorator(f):\n",
        "    @functools.wraps(f)\n",
        "    def wrapper(*args, **kwargs):\n",
        "      print(\"hello_0\")\n",
        "      compile_with_nvcc(kernel_file_without_cu)\n",
        "      print(\"hello\")\n",
        "      decorated = kernel_function(f\"{os.getcwd()}/{kernel_file_without_cu + '.o'}\")(f)\n",
        "      return decorated\n",
        "    return wrapper\n",
        "\n",
        "# I'll deal with this later\n",
        "# @make_kernel_function(\"add_kernel\")\n",
        "# def add_cuda(lib, a: torch.Tensor, c: torch.Tensor, size: int, stream: torch.cuda.streams.Stream) -> None:\n",
        "    # return lib.add_cuda"
      ],
      "metadata": {
        "id": "1tTswqR0BYmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python driver_tensor_sum.py"
      ],
      "metadata": {
        "id": "T-mPdqfHAvSn",
        "outputId": "2e759bbc-0bac-424f-e08c-96ab13a3b80b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compilation successful\n",
            "calling our kernel...\n",
            "tensor([4096.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones(1024).size(dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DmeM4Fb1YBy",
        "outputId": "b2aa76e8-f0e7-42fe-fb2d-6527eda9fa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import driver_tensor_sum\n",
        "\n",
        "driver_tensor_sum.basic_tensor_sum(torch.ones(1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trdcHVHNzu1d",
        "outputId": "8d02f27f-ffc7-41e8-8ca1-5ee4dfca6a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compilation successful\n",
            "calling our kernel...\n",
            "tensor([4096.], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1024.], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a performance pyramid - divide and conquer\n",
        "\n",
        "When building systems, user (and programmer) ease-of-use often needs to be traded off with performance. So, limiting the functionality in certain ways can often lead to performance boosts. As a performance engineer, you can use opportunities like this to improve your code's performance by processing the data in several passes using different APIs.\n",
        "\n",
        "For example, consider the atomicAdd function we used above for calculating the sum of the tensor. Every thread has to acquire a lock to ensure that no other threads are in the middle of operating on the data, perform the operation, and then release the lock so that other threads can operate on the data. Accessing global memory is also often fairly slow, as it can not take advantage of [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference). So, a way to speed up your kernel is by using cached data. CUDA allows us to allocate and use shared memory, which threads in a thread block can read and write to. We will use shared memory to reduce the number of atomic additions we have to perform in the following section."
      ],
      "metadata": {
        "id": "-Cm81RiCE3mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shared Memory\n",
        "\n",
        "Read [this article](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/) on using shared memory in your CUDA kernels, and use static shared memory to read blocks of 256 (the maximum size of a thread block) elements from global memory. Then, reduce the elements following a pattern like this (you are starting with 256 elements instead of 8 in this illustration):\n",
        "\n",
        "[![](https://mermaid.ink/img/pako:eNqF0lFrgzAQAOC_Eq5YO2hhJm3H8jBwtWvf28e8BE1XWY0ji2xD_O-NzYNBDSYvx_GR3B1XQ1pmAigEQZ3LXFNUh_oqChHSMOPqK2yaIGCSycut_E2vXGl0TphE5sTRIp7PXqP57I88m4A8odXqDcW4n7b83eHRNN85HHd8h_tpyxOHk2m-d_i643vcT1v-4fDNND84fNvxA-6nLT86_GWaDydmB0k8g8Sjg_TxYU-2VeJpFY-26uPDX20xa08xZLSYMd5eWEIhVMHzzCx03T7B4LHMDKgJ231mwGRjHK90efqXKVCtKrGE6jvjWiQ5_1S8AHrhtx_R3AFufrGG?type=png)](https://mermaid.live/edit#pako:eNqF0lFrgzAQAOC_Eq5YO2hhJm3H8jBwtWvf28e8BE1XWY0ji2xD_O-NzYNBDSYvx_GR3B1XQ1pmAigEQZ3LXFNUh_oqChHSMOPqK2yaIGCSycut_E2vXGl0TphE5sTRIp7PXqP57I88m4A8odXqDcW4n7b83eHRNN85HHd8h_tpyxOHk2m-d_i643vcT1v-4fDNND84fNvxA-6nLT86_GWaDydmB0k8g8Sjg_TxYU-2VeJpFY-26uPDX20xa08xZLSYMd5eWEIhVMHzzCx03T7B4LHMDKgJ231mwGRjHK90efqXKVCtKrGE6jvjWiQ5_1S8AHrhtx_R3AFufrGG)\n",
        "\n",
        "So, you add the odd-numbered elements to the even-numbered ones in step one, call `__syncthreads();` to wait for all threads, and repeat for all even threads, and so on. Make sure you call `__syncthreads();` in all threads in the block - the kernel will halt for all threads to call `__syncthreads();` before continuing execution.\n",
        "\n",
        "Advanced: use [memory fences](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-synchronization-domains) instead of the `__syncthreads();` thread barrier. Think about why would memory fences work here instead of `__syncthreads();`\n",
        "\n",
        "The first thread in all blocks can then write to the target using `atomicAdd`.\n",
        "\n",
        "Bonus: how does this compare to all blocks writing to global memory that is 256 times smaller, and the driver code calling the same kernel until it has been reduced completely? Is the `atomicAdd` approach faster in some cases?"
      ],
      "metadata": {
        "id": "Ugi87cEodcIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python driver_tensor_sum_tree_reduce.py"
      ],
      "metadata": {
        "id": "ucnNQMRSNwfQ",
        "outputId": "b60bafc6-b2e0-451d-c83e-8fcc70c2dcec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compilation successful\n",
            "calling our kernel...\n",
            "tensor([1.0240e+03, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00, 1.0000e+00,\n",
            "        1.0000e+00], device='cuda:0')\n",
            "b[0]=tensor(1024., device='cuda:0')\n",
            "b[256]=tensor(1., device='cuda:0')\n",
            "b[512]=tensor(1., device='cuda:0')\n",
            "b[768]=tensor(1., device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import driver_tensor_sum_tree_reduce\n",
        "\n",
        "driver_tensor_sum_tree_reduce.tree_tensor_sum(torch.ones(1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJXE02-k4caT",
        "outputId": "848180b7-bf4d-4af3-9603-d34db5ff1d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compilation successful\n",
            "calling our kernel...\n",
            "tensor([1.0240e+03, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00, 1.0000e+00,\n",
            "        1.0000e+00], device='cuda:0')\n",
            "b[0]=tensor(1024., device='cuda:0')\n",
            "b[256]=tensor(1., device='cuda:0')\n",
            "b[512]=tensor(1., device='cuda:0')\n",
            "b[768]=tensor(1., device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1024.], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced: Shuffling\n",
        "\n",
        "[Shuffling](https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/) lets you perform faster reductions - use this to calculate the sum in a warp before summing the thread block."
      ],
      "metadata": {
        "id": "aM-tdjPleUOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmarking your code\n",
        "\n",
        "Here's a helper function that runs your code 10 times to warm it up, and then 100 times to calculate the mean and std of execution times.\n",
        "\n",
        "Alternatively, you can use the %timeit magic. However, this does not let torch versions warm up, so you might bet weird results."
      ],
      "metadata": {
        "id": "v87Chjkq7bCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch as t\n",
        "\n",
        "def benchmark(fn):\n",
        "    for _ in range(10):\n",
        "        x = t.rand(256*256*256, device='cuda', dtype=t.float32)\n",
        "        fn(x)\n",
        "\n",
        "    times = []\n",
        "    for _ in range(100):\n",
        "        x = t.rand(256*256*256, device='cuda', dtype=t.float32)\n",
        "        start = time.perf_counter_ns()\n",
        "        fn(x)\n",
        "        times.append(time.perf_counter_ns() - start)\n",
        "    times = np.array(times, dtype=float)\n",
        "    return f\"{times.min()} ns; {times.max()} ns per invocation (min; max of 100 runs)\""
      ],
      "metadata": {
        "id": "hcXoLF9Z7cfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(benchmark(lambda x: x.sum()))  # baseline\n",
        "print(benchmark(lambda x: driver_tensor_sum.basic_tensor_sum(x)))\n",
        "print(benchmark(lambda x: driver_tensor_sum_tree_reduce.tree_tensor_sum(x)))"
      ],
      "metadata": {
        "id": "ZzYWUDrl7fTo",
        "outputId": "513f7299-df00-4249-8fc5-8ded03baa5a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11886.0 ns; 32381.0 ns per invocation (min; max of 100 runs)\n",
            "33315464.0 ns; 35268894.0 ns per invocation (min; max of 100 runs)\n",
            "2910779.0 ns; 10675611.0 ns per invocation (min; max of 100 runs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What's next\n",
        "\n",
        "You can find pytorch's implementations of cuda kernels [here](https://github.com/search?q=repo%3Apytorch%2Fpytorch+path%3Acuda%2F*.cu&type=code). Try figuring out how they work, or writing your own implementations and comparing the performance difference certain optimizations make. You can also read the [nvidia blog](https://developer.nvidia.com/blog/search-posts/?q=CUDA+C%2B%2B) to find out API features and how to use them.\n",
        "\n",
        "A fun kernel to implement is matrix multiplication - this forms the basis for a lot of operations in DL, and can be implemented in a bunch of different ways. A fun exercise is implementing the 4x4 matrix multiplication algorithm Deepmind's AlphaTensor found recently - it is currently the fastest known method of multiplying 4x4 tensors!"
      ],
      "metadata": {
        "id": "XO2gvfYBG25H"
      }
    }
  ]
}